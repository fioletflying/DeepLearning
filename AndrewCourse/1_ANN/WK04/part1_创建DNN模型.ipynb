{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建深度的DNN模型\n",
    "\n",
    "### 基础库和函数\n",
    "\n",
    "这里需要导入的库有：\n",
    "\n",
    "- numpy：这个不用介绍\n",
    "- h5py 用来分析H5 文件的包.\n",
    "- matplotlib 不用介绍\n",
    "- dnn_utils：作者给定的一些必要的函数：sigmoid, sigmoid_backward, relu, relu_backward\n",
    "- testCases 测试函数\n",
    "- np.random.seed(1) 保证随机数的确定性\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    输出：\n",
    "    A: 返回最终计算的值\n",
    "    cache:跟A一样的值，用来反向传播的使用\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1.0/(1.0+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A,cache\n",
    "\n",
    "def sigmoid_backward(dA,cache):\n",
    "    \"\"\"\n",
    "    dA:表示后一层的梯度值\n",
    "    cache:表示之前保存的前一层的缓存的值\n",
    "    \"\"\"\n",
    "    \n",
    "    # 这里将前一层缓存的值直接代入\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1.0/(1.0+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    输出：\n",
    "    A: 返回前向计算的值\n",
    "    cache:跟A一样的值，用来反向传播的使用\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0与Z比较\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    # 缓存该值用于反向传播计算\n",
    "    cache = Z\n",
    "    \n",
    "    return A,cache\n",
    "\n",
    "def relu_backward(dA,cache):\n",
    "    \"\"\"\n",
    "    dZ: 最终计算的梯度值\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础辅助函数的定义\n",
    "\n",
    "这里我们需要定义一些基础的辅助函数，这样是为了后面实现一个更具深度的模型建立。主要的建立的函数如下：\n",
    "\n",
    "- 参数初始化函数：2层网络与L层网络\n",
    "- 前向网络传播函数\n",
    "    - 线性传播层WX+b\n",
    "    - 激活函数：relu/sigmoid\n",
    "    - 组合上面[线性+激活]层的前向传播\n",
    "    - 堆叠[线性+relu激活]层的前向传播以及[线性+sigmoid激活]\n",
    "- loss 函数\n",
    "- 反向传播函数\n",
    "    - 线性反向传播层\n",
    "    - 激活函数的反向传播：relu/sigmoid\n",
    "    - 组合上面[线性+激活]层的反向传播\n",
    "    - 堆叠[线性+relu激活]层的反向传播以及[线性+sigmoid激活]\n",
    " - 参数更新函数\n",
    " \n",
    " 上面的实践函数的过程如下图：\n",
    " ![](imgs/1.jpg)\n",
    " \n",
    " 下面让我开始码代码吧。\n",
    " \n",
    " ### 参数初始化\n",
    " \n",
    " #### 2层网络的初始化\n",
    " \n",
    " 我们需要了解一下内容：\n",
    " - 模型的结果是：线性->relu激活函数->线性->sigmoid激活函数\n",
    " - W权重参数需要用到的numpy的函数是：np.random.randn(shape)*0.01\n",
    "     -  W权重参数可以使用符合高斯分布的随机数进行初始化：np.random.randn()*0.01\n",
    " - b偏置值参数直接复制为0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        n_x: 表示输入的数据的个数\n",
    "        n_h: 中间隐藏层的神经元的个数\n",
    "        n_y: 表示输出的数据的个数\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    # W1的shape是根据前一层和后一层的维度确定\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    # b的维度是根据当前层的维度来确定\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    # 辅助判断参数的shape\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters={\"W1\":W1,\n",
    "               \"b1\":b1,\n",
    "               \"W2\":W2,\n",
    "               \"b2\":b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "#测试函数\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L层网络的初始化\n",
    "\n",
    "如果需要对多层网络的初始化，这里我们就需要每层网络的隐藏网络神经元的个数。下面的图标中揭示如何计算权重与偏置的shape:\n",
    "![](imgs/2.jpg)\n",
    "\n",
    "上图中$n^l$就是第l层的神经元的个数。\n",
    "\n",
    "这里有一个例子关于WX+b的结果如下图：\n",
    "![](imgs/3.jpg)\n",
    "![](imgs/4.jpg)\n",
    "\n",
    "关于实践L层网络的coding我们要注意点：\n",
    "- 输入的一个数组layer_dims[2,4,1]\n",
    "    - 2: 表示第0层的神经元的个数，4:....\n",
    "    - W1(4,2),W2(1,4)\n",
    "    - b1(4,1),b2(1,1)\n",
    "    \n",
    "具体的coding如下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        layer_dims：是各个层的神经元的个数组成的数组\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    # 获得数组的程度\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # 从第二层开始\n",
    "    for l in range(1,L):\n",
    "        # 为了获得前一层的维度，需要减1\n",
    "        parameters['W'+str(l)] = np.random.randn(\n",
    "            layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((\n",
    "            layer_dims[l],1))\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播模型\n",
    "\n",
    "这里我们需要完成以下的基础函数：\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION ACTIVATION 要么是 ReLU 要么是 Sigmoid.\n",
    "- [LINEAR -> RELU] ×× (L-1) -> LINEAR -> SIGMOID (whole model)\n",
    "\n",
    "\n",
    "#### 线性前向传播\n",
    "\n",
    "这里的线性前向传播就是简单的权重与偏置值的相乘与相加。也就是如下图\n",
    "\n",
    "这里有一个例子关于WX+b的结果如下图：\n",
    "![](imgs/3.jpg)\n",
    "![](imgs/4.jpg)\n",
    "\n",
    "但是为了表示更加一般的表示，我们一般使用如下的方式：\n",
    "![](imgs/5.jpg)\n",
    "\n",
    "主要注意的是：\n",
    "- 输入层的X,我们用$A^{[0]}$表示\n",
    "- WX:这里 使用的np.dot来实践\n",
    "\n",
    "具体的coding如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        A:表示前一层的计算结果\n",
    "        W:表示当前层的权重\n",
    "        b:表示当前层的偏置\n",
    "        \n",
    "    输出：\n",
    "        Z: 计算结果\n",
    "        cache:A,W,b的数据\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性 - 激活函数的前向传播\n",
    "\n",
    "- Sigmoid激活函数：\n",
    "    - ”a“表示是激活函数的a\n",
    "    - \"cache\"表示缓存的上一层的输入值\n",
    "![](imgs/6.jpg)\n",
    "\n",
    "- RELU激活函数：\n",
    "![](imgs/7.jpg)\n",
    "\n",
    "这里是将线性 - 激活函数的组合，所以我们需要使用上面的线性函数。计算过程如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    A_prev:表示上一层的激活函数计算的值或者输入层数据\n",
    "    W,b:表述参数\n",
    "    activation： 表示当前激活函数样式：\"sigmoid\"&\"relu\"\n",
    "    \n",
    "    输出：\n",
    "    A:当前层的输出\n",
    "    cache:缓存linear_cache, activation_cache\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A,cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L 层模型前向传播\n",
    "\n",
    "这里的L层网络的前向传播，其实就是组合层一个模型，让数据从输入自然的经过各种组合最后输出。具体的过程如下图：\n",
    "![](imgs/8.jpg)\n",
    "\n",
    "从上图中我们可以看出：\n",
    "- AL是一直计算获得，前L-1获得\n",
    "- ![](imgs/9.jpg)\n",
    "- 同时我们这里仅仅需要对这个组合[LINEAR->RELU]进行(L-1)次\n",
    "具体的coding如下\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        X: 输入的数据\n",
    "        parameters:具体的参数，\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    # 通过参数就可以获得模型的层数\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # 前L-1层的网络\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = linear_activation_forward(A_prev,\n",
    "                            parameters['W'+str(l)],\n",
    "                            parameters['b'+str(l)],\n",
    "                                           'relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # 最后一层的网络   \n",
    "    AL,cache = linear_activation_forward(A,\n",
    "                            parameters['W'+str(L)],\n",
    "                            parameters['b'+str(L)],\n",
    "                                        'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss函数\n",
    "\n",
    "使用cross-entropy来计算损失函数：\n",
    "![](imgs/10.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        AL:最后一层的输出值\n",
    "        Y:标签数据\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    # j计算loss值\n",
    "    cost = -np.sum(np.muliply(np.log(AL),Y)+\n",
    "                   np.multiply(np.log(1-AL),1-Y))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播模型\n",
    "\n",
    "反向传播为了将loss传递到每一层的参数，从而更新参数的数据。具体的步骤如下图所示：\n",
    "![](imgs/11.jpg)\n",
    "\n",
    "这里有一个比较关键的反向传播过程，那就是第一层的dz1的计算：\n",
    "![](imgs/12.jpg)\n",
    "\n",
    "下面我们同前向传播一样，我们需要做创建如下的基础函数：\n",
    "- LINEAR 反向传播\n",
    "- LINEAR -> ACTIVATION 反向传播，relu/sigmoid\n",
    "- [LINEAR -> RELU] ×× (L-1) -> LINEAR -> SIGMOID 反向传播\n",
    "\n",
    "#### 线性的反向传播函数\n",
    "\n",
    "假设第l层，其线性前向传播如下:\n",
    "![](imgs/5.jpg)\n",
    "\n",
    "需要计算的示意图如下：\n",
    "![](imgs/13.jpg)\n",
    "\n",
    "具体的计算公式如下：\n",
    "![](imgs/14.jpg)\n",
    "\n",
    "如下coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        dZ 前一层的Activate导数\n",
    "        cache: 该层缓存的（A_prev,W,b）\n",
    "    \n",
    "    输出：\n",
    "        dA_prev：loss的梯度\n",
    "        dW:W的梯度\n",
    "        db:b的梯度\n",
    "    \"\"\"\n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev,dW,db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[1.01258895]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性-激活函数的反向传播\n",
    "\n",
    "这是需要跟刚刚的线性反向传播组合在一起使用，所以我们需要做获得sigmoid_backward 与 relu_backward函数组合。\n",
    "这里需要使用的计算公式是：\n",
    "![](imgs/15.jpg)\n",
    "\n",
    "具体的coding如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        dA: 前一层的梯度\n",
    "        cache: 前向传播缓存的值：linear_cache,activation_catche\n",
    "        activation: 激活函数的形式\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.11459244]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.41675785]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L 层网络的反向传播\n",
    "\n",
    "关于L层网络的反向传播，是对整个网络的组合在一起，再进行梯度训练。所以这里我们需要时刻要注意的是需要利用之前前向传播缓存的数据(X,W,b,z)，然后再迭代完成每一层的梯度计算。具体的流程如下图：\n",
    "![](imgs/16.jpg)\n",
    "\n",
    "**loss函数的梯度计算**  \n",
    "\n",
    "从上面的图中我们这知道反向传播是从最后一层开始的，也就是计算loss值后开始计算的。所以需要计算loss值的反向传播。\n",
    "线拿到loss值得函数：\n",
    "![](imgs/10.jpg)\n",
    "\n",
    "计算上面得dAL得值也比较简单：  \n",
    "`dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))`  \n",
    "\n",
    "**最后一层linear-activation的计算**  \n",
    "从上面的图中我们可以看到，在获得output层的梯度dAL后，这里我们需要需要计算的是一个linear-sigmoid层。  \n",
    "\n",
    "**循环层linear-relu的计算**  \n",
    "后面的数据都是关于linear-relu循环层的计算，所以比较简单。当然每个层计算的梯度都需要按一定的格式记录保存。    \n",
    "$grads[^\"dW^\"+str(l)]=dW^{[l]}$  \n",
    "\n",
    "- l 表示第几层\n",
    "- grads表示是一个字典数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # 这里计算loss function的梯度\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # 计算最后一层linear-sigmoid的梯度\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L)],grads[\"dW\"+str(L)],grads[\"db\"+str(L)]=linear_activation_backward(dAL, \n",
    "                                                                                        current_cache,\"sigmoid\")\n",
    "    \n",
    "    # 迭代计算其他层linear-relu的梯度\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\"+str(l+1)],grads[\"dW\"+str(l+1)],grads[\"db\"+str(l+1)]=linear_activation_backward(grads[\"dA\"+str(l+2)],\n",
    "                                                                                            current_cache,\"relu\")\n",
    "             \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.44014127]\n",
      " [ 0.        ]\n",
      " [-0.05670698]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数更新\n",
    "参数更新的公式如下：\n",
    "![](imgs/17.jpg)\n",
    "\n",
    "这里只需要设置一个超参数$\\alpha$值。需要主要的是，我们需要通过迭代对每一层的参数进行更新。具体的coding如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
