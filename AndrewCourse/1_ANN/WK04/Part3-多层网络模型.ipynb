{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层网络的实践案例\n",
    "\n",
    "### 基础库\n",
    "这里需要导入的库有：\n",
    "\n",
    "- numpy：这个不用介绍\n",
    "- h5py 用来分析H5 文件的包.\n",
    "- matplotlib 不用介绍\n",
    "- PIL and scipy 用来分析最后的预测结果.\n",
    "- dnn_utils：作者给定的一些必要的函数：sigmoid, sigmoid_backward, relu, relu_backward\n",
    "- testCases 测试函数\n",
    "- np.random.seed(1) 保证随机数的确定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "\n",
    "这里的问题就是想通过一个模型来判别给出的图片是否为猫。\n",
    "- 图片的shape(width,hight,3),RGB的图片\n",
    "- 这里的图片数据是保存在一个“data.h5”的文件中\n",
    "\n",
    "现在开始来获取训练集和测试集。就是利用上面的给的脚本函数：\n",
    "- train_set_x_orig ： 训练集orig,表示原始数据，后面我们要对原始数据进行归一化等处理\n",
    "- traing_set_y：训练集标签\n",
    "- test_set_x_orig：测试集orig,表示原始数据\n",
    "- test_set_y：测试集标签\n",
    "- classes：种类\n",
    "\n",
    "这些数据的格式通过shape函数就可以获得：\n",
    "- train_set_x_orig：(209, 64, 64, 3)\n",
    "    - 209：表示训练样本的个数\n",
    "    - (64, 64, 3):表示图片size 和channel\n",
    "- traing_set_y：(1, 209)\n",
    "- test_set_x_orig：(50, 64, 64, 3)\n",
    "- test_set_y：(1, 50)\n",
    "- classes：(2,)\n",
    "    - non-cat\n",
    "    - cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(12288, 50)\n",
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "#获取数据集\n",
    "train_set_x_orig,train_set_y,test_set_x_orig,test_set_y, classes= load_data()\n",
    "\n",
    "# 变换数据\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "# 这里的.T表示转置，为了与后面的W相匹配，也方便大家理解输入的神经元\n",
    "train_set_x_flat = train_set_x_orig.reshape(m_train,-1).T\n",
    "test_set_x_flat = test_set_x_orig.reshape(m_test,-1).T\n",
    "\n",
    "print(train_set_x_flat.shape)\n",
    "print(test_set_x_flat.shape)\n",
    "\n",
    "#### 数据的归一化\n",
    "train_x = train_set_x_flat/255.\n",
    "test_x = test_set_x_flat/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L层网络的模型结构\n",
    "\n",
    "其结构图如下：\n",
    "![](imgs/19.jpg)\n",
    "\n",
    "- 输入时(64,64,3)的图片，拉伸转换成一个列向量(12288,1)\n",
    "- 这里的第一层网络隐层层为n1,那么权重W1 shape为(n1,12288)\n",
    "- 经过第一层的relu激活函数后，A1就变成(n1,1)\n",
    "- 循环执行上面两步创建更多的层，\n",
    "- 最后经过一层sigmoid激活函数，输出该值。\n",
    "\n",
    "### 创建模型的一般步骤\n",
    "\n",
    "- 初始化参数并定义超参数\n",
    "- 循环更新：\n",
    "    - 前向传播\n",
    "    - 计算损失值\n",
    "    - 反向传播\n",
    "    - 更新参数\n",
    "- 使用训练的数据预测\n",
    "\n",
    "### 多层神经网络的创建\n",
    "\n",
    "其结果的流程如下：\n",
    "[线性-> Relu激活]x(L-1) -> 线性 -> Sigmoid 激活\n",
    "需要使用上一节定义的一些基础函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        n_x: 表示输入的数据的个数\n",
    "        n_h: 中间隐藏层的神经元的个数\n",
    "        n_y: 表示输出的数据的个数\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    # W1的shape是根据前一层和后一层的维度确定\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    # b的维度是根据当前层的维度来确定\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    # 辅助判断参数的shape\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters={\"W1\":W1,\n",
    "               \"b1\":b1,\n",
    "               \"W2\":W2,\n",
    "               \"b2\":b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        layer_dims：是各个层的神经元的个数组成的数组\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    # 获得数组的程度\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # 从第二层开始\n",
    "    for l in range(1,L):\n",
    "        # 为了获得前一层的维度，需要减1\n",
    "        parameters['W'+str(l)] = np.random.randn(\n",
    "            layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((\n",
    "            layer_dims[l],1))\n",
    "        \n",
    "    return parameters\n",
    "    \n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        A:表示前一层的计算结果\n",
    "        W:表示当前层的权重\n",
    "        b:表示当前层的偏置\n",
    "        \n",
    "    输出：\n",
    "        Z: 计算结果\n",
    "        cache:A,W,b的数据\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    cache = (A,W,b)\n",
    "    \n",
    "    return Z,cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    A_prev:表示上一层的激活函数计算的值或者输入层数据\n",
    "    W,b:表述参数\n",
    "    activation： 表示当前激活函数样式：\"sigmoid\"&\"relu\"\n",
    "    \n",
    "    输出：\n",
    "    A:当前层的输出\n",
    "    cache:缓存linear_cache, activation_cache\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A,cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        X: 输入的数据\n",
    "        parameters:具体的参数，\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    # 通过参数就可以获得模型的层数\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # 前L-1层的网络\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = linear_activation_forward(A_prev,\n",
    "                            parameters['W'+str(l)],\n",
    "                            parameters['b'+str(l)],\n",
    "                                           'relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # 最后一层的网络   \n",
    "    AL,cache = linear_activation_forward(A,\n",
    "                            parameters['W'+str(L)],\n",
    "                            parameters['b'+str(L)],\n",
    "                                        'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        AL:最后一层的输出值\n",
    "        Y:标签数据\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    # j计算loss值\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y)+\n",
    "                   np.multiply(np.log(1-AL),1-Y))/m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        dZ 前一层的Activate导数\n",
    "        cache: 该层缓存的（A_prev,W,b）\n",
    "    \n",
    "    输出：\n",
    "        dA_prev：loss的梯度\n",
    "        dW:W的梯度\n",
    "        db:b的梯度\n",
    "    \"\"\"\n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev,dW,db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        dA: 前一层的梯度\n",
    "        cache: 前向传播缓存的值：linear_cache,activation_catche\n",
    "        activation: 激活函数的形式\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    return dA_prev,dW,db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # 这里计算loss function的梯度\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # 计算最后一层linear-sigmoid的梯度\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L)],grads[\"dW\"+str(L)],grads[\"db\"+str(L)]=linear_activation_backward(dAL, \n",
    "                                                                                        current_cache,\"sigmoid\")\n",
    "    \n",
    "    # 迭代计算其他层linear-relu的梯度\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\"+str(l+1)],grads[\"dW\"+str(l+1)],grads[\"db\"+str(l+1)]=linear_activation_backward(grads[\"dA\"+str(l+2)],\n",
    "                                                                                            current_cache,\"relu\")\n",
    "             \n",
    "    return grads\n",
    "    \n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        parameters：缓存的参数字典\n",
    "            parameters['W' + str(l)] = Wl\n",
    "            parameters['b' + str(l)] = bl\n",
    "        grads: 缓存的梯度字典\n",
    "            grads['dW' + str(l)] = dWl\n",
    "            grads['db' + str(l)] = dbl\n",
    "        learning_rate：学习率\n",
    "    \"\"\"\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L): # 从零开始，而参数是从1开始W1\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5层网络模型\n",
    "layers_dims = [12288, 20, 7, 5, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    构建一个两层神经网络模型: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "    输入:\n",
    "    X -- 输入数据\n",
    "    Y -- 标签数据 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- 网络的结构维度 (n_x, n_h, n_y)\n",
    "    num_iterations -- 循环优化的次数\n",
    "    learning_rate -- 学习率\n",
    "    print_cost -- 是否需要每隔100次打印cost值 \n",
    "\n",
    "    输出:\n",
    "    parameters -- 字典 W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # 基础参数\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # 开始迭代更新\n",
    "    for i in range(0,num_iterations):\n",
    "        \n",
    "        AL,caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # 计算loss值\n",
    "        cost = compute_cost(AL,Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, \n",
    "                                       grads,\n",
    "                                       learning_rate)\n",
    "        \n",
    "        # 打印loss 值\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # 绘制cost图\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.678011\n",
      "Cost after iteration 200: 0.667600\n",
      "Cost after iteration 300: 0.660422\n",
      "Cost after iteration 400: 0.655458\n",
      "Cost after iteration 500: 0.652013\n",
      "Cost after iteration 600: 0.649616\n",
      "Cost after iteration 700: 0.647942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c2a67dd004be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                              \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                              \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                              print_cost=True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-45e353827bd4>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         parameters = update_parameters(parameters, \n",
      "\u001b[1;32m<ipython-input-3-85deeffa5697>\u001b[0m in \u001b[0;36mL_model_backward\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         grads[\"dA\"+str(l+1)],grads[\"dW\"+str(l+1)],grads[\"db\"+str(l+1)]=linear_activation_backward(grads[\"dA\"+str(l+2)],\n\u001b[1;32m--> 199\u001b[1;33m                                                                                             current_cache,\"relu\")\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-85deeffa5697>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[1;34m(dA, cache, activation)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdA_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-85deeffa5697>\u001b[0m in \u001b[0;36mlinear_backward\u001b[1;34m(dZ, cache)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mdA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_set_y,\n",
    "                             layers_dims,\n",
    "                             num_iterations = 2500, \n",
    "                             print_cost=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
