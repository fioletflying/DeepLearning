{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow注入机制\n",
    "\n",
    "- 使用placeholder用于占位符，\n",
    "- 使用feed_dict将数据注入到占位符中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add: 7\n",
      "mul: 12\n",
      "[7, 12]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow注入机制\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "add = tf.add(a,b)\n",
    "mul = tf.multiply(a,b)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 使用feed_dict 注入数据，由于使用了placeholder\n",
    "# 将数据注入到placeholder中\n",
    "print(\"add: %i\" % sess.run(add,feed_dict={a:3,b:4}))\n",
    "print(\"mul: %i\" % sess.run(mul,feed_dict={a:3,b:4}))\n",
    "print(sess.run([add,mul],feed_dict={a:3,b:4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [0.6782267]  b= [0.35785705]\n",
      "Epoch: 3  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.5893728]  b= [0.11283745]\n",
      "Epoch: 5  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.8363491]  b= [0.01982224]\n",
      "Epoch: 7  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9004006]  b= [-0.00472769]\n",
      "Epoch: 9  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9169658]  b= [-0.01108396]\n",
      "Epoch: 11  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9212494]  b= [-0.01272771]\n",
      "Epoch: 13  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9223565]  b= [-0.0131526]\n",
      "Epoch: 15  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9226437]  b= [-0.01326276]\n",
      "Epoch: 17  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9227176]  b= [-0.01329114]\n",
      "Epoch: 19  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.9227365]  b= [-0.01329838]\n",
      "Finished!\n",
      " cost= 0.08545048  W= [1.9227399]  b= [-0.01329968]\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "\n",
    "# 准备数据\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 生成一个-1,1的等差数列\n",
    "X_train = np.linspace(-1,1,100)\n",
    "Y_train = 2*X_train + np.random.randn(*X_train.shape)*0.3\n",
    "\n",
    "# 重置图\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#创建模型\n",
    "#占位符:用来存放训练的数据\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "# 模型的参数\n",
    "# W生成一个随机数[-1,1],形状为一维的\n",
    "W = tf.Variable(tf.random_normal([1]),name=\"weight\") \n",
    "b = tf.Variable(tf.zeros([1]),name=\"bias\")\n",
    "# 前向传播的结构\n",
    "z = tf.multiply(X,W)+b\n",
    "\n",
    "# 反向传播计算梯度以及优化\n",
    "# 定义损失函数为平方差\n",
    "cost = tf.reduce_mean(tf.square(Y-z))\n",
    "# 设定学习率\n",
    "learning_rate = 0.01\n",
    "# 选择梯度下降算法\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#初始化所有的参数\n",
    "init = tf.global_variables_initializer()\n",
    "iter_num = 20\n",
    "display_step = 2\n",
    "\n",
    "#######保存模型的路径###########\n",
    "saver = tf.train.Saver()\n",
    "savedir = \"models/\"\n",
    "\n",
    "sess.run(init)\n",
    "plotdata = {\"batchsize\":[],\"loss\":[]}\n",
    "for epoch in range(iter_num):\n",
    "    for (x,y) in zip(X_train,Y_train):\n",
    "        sess.run(optimizer,feed_dict = {X:x,Y:y})\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        loss = sess.run(cost,feed_dict={X:X_train,Y:Y_train})\n",
    "        print(\"Epoch:\",epoch+1,\" cost=\",cost,\" W=\",sess.run(W),\" b=\",sess.run(b))\n",
    "\n",
    "\n",
    "print(\"Finished!\")\n",
    "######保存模型########\n",
    "saver.save(sess,savedir+\"linearModel.cpkt\")\n",
    "\n",
    "print(\" cost=\",sess.run(cost,feed_dict={X:X_train,Y:Y_train}),\" W=\",sess.run(W),\" b=\",sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的保存于载入\n",
    "\n",
    "模型的文件形式：\n",
    "\n",
    "- checkpoint文件会记录保存信息，通过它可以定位最新保存的模型：\n",
    "- .meta文件保存了当前图结构\n",
    "- .index文件保存了当前参数名\n",
    "- .data文件保存了当前参数值\n",
    "\n",
    "- Meta graph:\n",
    "    - 这是一个协议缓冲区(protocol buffer)，它完整地保存了Tensorflow图；即所有的变量、操作、集合等。此文件以 .meta 为拓展名。\n",
    "\n",
    "- Checkpoint 文件：\n",
    "    - 这是一个二进制文件，包含weights、biases、gradients 和其他所有变量的值。此文件以 .ckpt 为扩展名. 但是，从Tensorflow 0.11版本之后做出了一些改变。现在，不再是单一的 .ckpt 文件，而是一下两个文件：\n",
    "    \n",
    "    ![](imgs/1.jpg)\n",
    "    \n",
    "    \n",
    " #### 保存模型函数\n",
    "  \n",
    " - tf.train.Saver() 创建一个保存的变量\n",
    "    - var_list\tSaver中存储变量集合\t全局变量集合\n",
    "    - reshape\t加载时是否恢复变量形状\tTrue\n",
    "    - sharded \t是否将变量轮循放在所有设备上\tTrue\n",
    "    - max_to_keep\t保留最近检查点个数\t5\n",
    "    - restore_sequentially\t 是否按顺序恢复变量，模型较大时顺序恢复内存消耗 True\n",
    "    - keep_checkpoint_every_n_hours 隔几个小时保存一次\n",
    "    \n",
    " - saver.save()  保存模型\n",
    " - tf.train.Saver.restore() ：加载模型数据\n",
    " \n",
    " #### 引入一个pretrained模型：\n",
    " \n",
    " - 创建网络：saver = tf.train.import_meta_graph('my_test_model-1000.meta')\n",
    " \n",
    " - 加载参数： 可以通过调用tf.train.Saver()类的restore方法来加载参数\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/linearModel.cpkt\n",
      "x=0.2,z= [0.3712483]\n"
     ]
    }
   ],
   "source": [
    "#模型的载入\n",
    "\n",
    "# 重置图\n",
    "#tf.reset_default_graph()\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    ####模型的载入，创建一个Session,然后再指出加载的文件路径####\n",
    "    saver.restore(sess2,savedir+\"linearModel.cpkt\")\n",
    "    print(\"x=0.2,z=\",sess2.run(z,feed_dict={X:0.2}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  bias\n",
      "[-0.01329968]\n",
      "tensor_name:  weight\n",
      "[1.9227399]\n"
     ]
    }
   ],
   "source": [
    "# 查看模型的保存内容\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file \n",
    "\n",
    "savedir = \"models/\"\n",
    "\n",
    "print_tensors_in_checkpoint_file(savedir+\"linearModel.cpkt\",None,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [0.9877056]  b= [0.35317853]\n",
      "Epoch: 3  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.779547]  b= [0.1456653]\n",
      "Epoch: 5  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [1.994774]  b= [0.06469445]\n",
      "Epoch: 7  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.0506015]  b= [0.0432982]\n",
      "Epoch: 9  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.0650396]  b= [0.03775795]\n",
      "Epoch: 11  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.068773]  b= [0.03632535]\n",
      "Epoch: 13  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.0697384]  b= [0.03595479]\n",
      "Epoch: 15  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.0699878]  b= [0.03585916]\n",
      "Epoch: 17  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.070053]  b= [0.03583424]\n",
      "Epoch: 19  cost= Tensor(\"Mean:0\", shape=(), dtype=float32)  W= [2.070069]  b= [0.03582808]\n",
      "Finished!\n",
      " cost= 0.09457235  W= [2.0700722]  b= [0.03582692]\n"
     ]
    }
   ],
   "source": [
    "# 保存检查点\n",
    "\n",
    "# 准备数据\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 生成一个-1,1的等差数列\n",
    "X_train = np.linspace(-1,1,100)\n",
    "Y_train = 2*X_train + np.random.randn(*X_train.shape)*0.3\n",
    "\n",
    "# 重置图\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#创建模型\n",
    "#占位符:用来存放训练的数据\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "# 模型的参数\n",
    "# W生成一个随机数[-1,1],形状为一维的\n",
    "W = tf.Variable(tf.random_normal([1]),name=\"weight\") \n",
    "b = tf.Variable(tf.zeros([1]),name=\"bias\")\n",
    "# 前向传播的结构\n",
    "z = tf.multiply(X,W)+b\n",
    "\n",
    "# 反向传播计算梯度以及优化\n",
    "# 定义损失函数为平方差\n",
    "cost = tf.reduce_mean(tf.square(Y-z))\n",
    "# 设定学习率\n",
    "learning_rate = 0.01\n",
    "# 选择梯度下降算法\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#初始化所有的参数\n",
    "init = tf.global_variables_initializer()\n",
    "iter_num = 20\n",
    "display_step = 2\n",
    "\n",
    "#######保存模型的路径###########\n",
    "saver = tf.train.Saver()\n",
    "savedir = \"models/\"\n",
    "\n",
    "sess.run(init)\n",
    "plotdata = {\"batchsize\":[],\"loss\":[]}\n",
    "for epoch in range(iter_num):\n",
    "    for (x,y) in zip(X_train,Y_train):\n",
    "        sess.run(optimizer,feed_dict = {X:x,Y:y})\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        loss = sess.run(cost,feed_dict={X:X_train,Y:Y_train})\n",
    "        print(\"Epoch:\",epoch+1,\" cost=\",cost,\" W=\",sess.run(W),\" b=\",sess.run(b))\n",
    "    \n",
    "    \n",
    "    ####在模型训练的时候就保存模型######\n",
    "    # 每一epoch保存一次\n",
    "    saver.save(sess,savedir+\"linearmodel.cpkt\",global_step=epoch)\n",
    "    \n",
    "    #########END##########\n",
    "\n",
    "\n",
    "print(\"Finished!\")\n",
    "######保存模型########\n",
    "saver.save(sess,savedir+\"linearModel.cpkt\")\n",
    "\n",
    "print(\" cost=\",sess.run(cost,feed_dict={X:X_train,Y:Y_train}),\" W=\",sess.run(W),\" b=\",sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/linearModel.cpkt\n",
      "x=0.2,z= [0.44984135]\n"
     ]
    }
   ],
   "source": [
    "###载入保存的检查点的结果###\n",
    "load_epock = 16\n",
    "saver = tf.train.Saver()\n",
    "savedir = \"models/\"\n",
    "\n",
    "with tf.Session() as sess2:\n",
    "    sess2.run(tf.global_variables_initializer())\n",
    "    # 方法一：\n",
    "    # saver.restore(sess2,savedir+\"linearmodel.cpkt-\"+str(load_epock))\n",
    "    # 方法二：\n",
    "    kpt = tf.train.latest_checkpoint(savedir)\n",
    "    if kpt != None:\n",
    "        saver.restore(sess2,kpt)\n",
    "        print(\"x=0.2,z=\",sess2.run(z,feed_dict={X:0.2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享变量的机制\n",
    "\n",
    "需要使用的函数;\n",
    "- tf.get_variable(<name>, <shape>, <initializer>):\n",
    "    - 通过所给的名字创建是返回一个变量\n",
    "- tf.variable_scope(<scope_name>): 通过 tf.get_variable()为变量名指定命名空间.\n",
    "    \n",
    " 为什么需要使用共享变量的机制：\n",
    " \n",
    " 我的理解是这样的，利用get_variable来代替variable，一方面可以使得参数的唯一性，另一方面，利用了共享机制后，优点像该变量成为了一个全局变量，这样就可以方便在任何地方使用，而不需要重新创建变量。\n",
    " 尤其在多个模型的时候，由于其参数是相同的，只是模型有差异，这时候如果两个不同的模型需要使用同一个参数，就会体现了共享变量的优势了。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: test1/firstvar:0\n",
      "var2: test1/test2/firstvar:0\n",
      "var3: test1/firstvar:0\n",
      "var4: test1/test2/firstvar:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 重置图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"test1\",): # 这里指定命名空间\n",
    "    # 在这个命名空间下定义一个get_variable\n",
    "    var1 = tf.get_variable(\"firstvar\",shape=[2],dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(\"test2\"):\n",
    "        # 在不同的命名空间下可以定义其他空间下已经命名过的名字\n",
    "        var2 = tf.get_variable(\"firstvar\",shape=[2],dtype=tf.float32)\n",
    "\n",
    "# 使用了reuse 为True\n",
    "# 会让变量共享，而不是重新新建是个变量\n",
    "with tf.variable_scope(\"test1\",reuse=True):\n",
    "    var3 = tf.get_variable(\"firstvar\",shape=[2],dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(\"test2\"):\n",
    "        var4 = tf.get_variable(\"firstvar\",shape=[2],dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "print(\"var1:\",var1.name)\n",
    "print(\"var2:\",var2.name)\n",
    "print(\"var3:\",var3.name)\n",
    "print(\"var4:\",var4.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
