{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "978a6c862e5d5032ad32c61f6e48b36d1a3d5930a41ca1b1b876a672583e57b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "source": [
    "### GradientTape\n",
    "\n",
    "y = 2*x*x^T\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[0.]\n [1.]\n [2.]\n [3.]], shape=(4, 1), dtype=float32)\n(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 设置变量\n",
    "x = tf.reshape(tf.Variable(range(4),dtype=tf.float32),(4,1))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[ 0.]\n [ 4.]\n [ 8.]\n [12.]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GradientTape 就是开启自动微分机制\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x) # 需要求解的变量\n",
    "    y = 2 * tf.matmul(tf.transpose(x),x)\n",
    "\n",
    "dy_dx = t.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf.Tensor(\n",
      "[[  0.]\n",
      " [  4.]\n",
      " [ 32.]\n",
      " [108.]], shape=(4, 1), dtype=float32) tf.Tensor(\n",
      "[[0.]\n",
      " [2.]\n",
      " [4.]\n",
      " [6.]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GradientTape 就是开启自动微分机制\n",
    "# 实现预测的模型\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x) # 需要求解的变量\n",
    "    y = x*x\n",
    "    z =y*y\n",
    "    dz_dx= g.gradient(z,x)\n",
    "    dy_dx = g.gradient(y,x)\n",
    "\n",
    "print(dz_dx,dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(-2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    y = a*tf.pow(x,2.0)+b*x+c\n",
    "\n",
    "dy_dx = gt.gradient(y,x)\n",
    "print(dy_dx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    # 这里控制相关的输入变量\n",
    "    gt.watch([a,b,c])\n",
    "    y = a*tf.pow(x,2.0)+b*x+c\n",
    "\n",
    "dy_dx,dy_da,dy_db,dy_dc = gt.gradient(y,[x,a,b,c])\n",
    "dy_dx,dy_da,dy_db,dy_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#二阶求导\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as gt_sec:\n",
    "    with tf.GradientTape() as gt_fir:\n",
    "        y = a*tf.pow(x,2.0)+b*x+c\n",
    "    dy_dx = gt_fir.gradient(y,x)\n",
    "dy2_dx2 = gt_sec.gradient(dy_dx,x)\n",
    "\n",
    "dy_dx,dy2_dx2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None)\n(<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>, None)\n"
     ]
    }
   ],
   "source": [
    "# 使用autograph 来进行自动微分\n",
    "@tf.function\n",
    "def autoGradientFunc(x):\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)   \n",
    "\n",
    "    x = tf.cast(x,tf.float32)\n",
    "    with tf.GradientTape() as gt:\n",
    "        gt.watch(x)\n",
    "        y = a*tf.pow(x,2.0)+b*x+c\n",
    "    dy_dx = gt.gradient(x,y)\n",
    "\n",
    "    return x,dy_dx\n",
    "\n",
    "\n",
    "print(autoGradientFunc(x=2))\n",
    "print(autoGradientFunc(x=-2))\n",
    "\n"
   ]
  },
  {
   "source": [
    "### 梯度下降来寻找最小值"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x= 0.867380381\n",
      "y= 0.0183131695\n"
     ]
    }
   ],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用梯度下降来寻找最小值\n",
    "\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "# 迭代100次来寻找最小值\n",
    "# 更新x = x0-dy_dx * learning_rate\n",
    "learning_rate = tf.constant(0.01)\n",
    "for _ in range(100):\n",
    "    #第一步计算梯度需要更新的值的梯度值\n",
    "    with tf.GradientTape() as gt:\n",
    "        y = a*tf.pow(x,2.0)+b*x+c\n",
    "    dy_dx = gt.gradient(y,x)\n",
    "\n",
    "    # 第二步 更新x = x0-dy_dx * learning_rate\n",
    "    # assign 是tf中给变量赋值的一种方法\n",
    "    x.assign(x - dy_dx * learning_rate)\n",
    "    # tf.print(dy_dx)\n",
    "tf.print(\"x=\",x)\n",
    "tf.print(\"y=\",y)\n",
    "    "
   ]
  },
  {
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "# 迭代1000次来寻找最小值\n",
    "# 更新x = x0-dy_dx * learning_rate\n",
    "\n",
    "learning_rate = tf.constant(0.01)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as gt:\n",
    "        y = a*tf.pow(x,2.0)+b*x+c\n",
    "    dy_dx = gt.gradient(y,x)\n",
    "    # 使用optimizer.apply_gradients\n",
    "    # 更新x = x0-dy_dx * learning_rate\n",
    "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "    # x.assign(x - dy_dx * learning_rate)\n",
    "    tf.print(dy_dx)\n",
    "    tf.print(\"x=\",x)\n",
    "    tf.print(\"y=\",y)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y= 0.0175879598   x= 0.867380381\n"
     ]
    }
   ],
   "source": [
    "# 调用optimizer.minimize优化器了求解\n",
    "# 这里就将自动微分和反向传播给自动更新了\n",
    "\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "#注意f()无参数\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for _ in range(100):\n",
    "    optimizer.minimize(f,[x])\n",
    "tf.print(\"y=\",f(),\"  x=\",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0175879\n",
      "0.867380381\n"
     ]
    }
   ],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "x = tf.Variable(0.0,name=\"x\",dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "\n",
    "    #注意autograph时使用tf.range(1000)而不是range(1000)\n",
    "    for _ in tf.range(100):\n",
    "        with tf.GradientTape() as gt:\n",
    "            y = a*tf.pow(x,2)+b*x+c\n",
    "        dy_dx = gt.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "\n",
    "    # 如果去掉这个就会出现问题，这里的y必须定义\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return y\n",
    "\n",
    "tf.print(minimizef())\n",
    "tf.print(x)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "0.999998569\n"
     ]
    }
   ],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "# 使用optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "@tf.function\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "# 这里就是进行训练\n",
    "@tf.function\n",
    "def train(epoch):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    return(f())\n",
    "\n",
    "\n",
    "tf.print(train(1000))\n",
    "tf.print(x)\n"
   ]
  }
 ]
}