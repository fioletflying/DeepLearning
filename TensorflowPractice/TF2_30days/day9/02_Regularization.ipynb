{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e17394781024da1f0ed279aab7c6165e80e3acfd2491de02c017c43da5039587"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Regularization\n",
    "\n",
    "上一个小节提到了，现在的网络的主要的问题是出现过拟合，目前我们可以对过拟合进行处理的方式有：\n",
    "\n",
    "- more data： 更多的数据\n",
    "- shallow model: 使用浅层的网络\n",
    "- Regularization： 正则化，是的复杂网络更加简单\n",
    "- Dropout\n",
    "- data argumention：数据增强\n",
    "- early stopping: 提前终止网络训练\n",
    "- ...\n",
    "\n",
    "那么什么是正则化的，正则化就是为了减少模型的复杂度来减少过拟合的一种方式。其原理，就是在loss函数中增加了一个约束，这个约束就是限制这个目标函数不能将参数来过大的变化，这样就是导致我们训练的参数限制在一个合理的范围里面。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets,layers,optimizers,Sequential,metrics\n",
    "\n",
    "def preprocess(x,y):\n",
    "    x = tf.cast(x,dtype=tf.float32)/255.\n",
    "    y =tf.cast(y,dtype=tf.int32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n"
     ]
    }
   ],
   "source": [
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n",
    "\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 256)               200960    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 244,522\nTrainable params: 244,522\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(128, activation='relu'),\n",
    "                     layers.Dense(64, activation='relu'),\n",
    "                     layers.Dense(32, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "optimizer = optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 loss: 2.3405680656433105 loss_regularization: 349.9940490722656\n",
      "78 Evaluate Acc: 0.1824\n",
      "100 loss: 0.32610294222831726 loss_regularization: 563.30615234375\n",
      "200 loss: 0.19031131267547607 loss_regularization: 643.8613891601562\n",
      "300 loss: 0.16033883392810822 loss_regularization: 720.5924682617188\n",
      "400 loss: 0.33136171102523804 loss_regularization: 791.5477905273438\n",
      "500 loss: 0.2671230733394623 loss_regularization: 855.9791259765625\n",
      "78 Evaluate Acc: 0.9542\n",
      "600 loss: 0.26694703102111816 loss_regularization: 887.657958984375\n",
      "700 loss: 0.25909659266471863 loss_regularization: 927.4588012695312\n",
      "800 loss: 0.22581970691680908 loss_regularization: 964.7063598632812\n",
      "900 loss: 0.16775313019752502 loss_regularization: 1002.3997802734375\n",
      "1000 loss: 0.1411725878715515 loss_regularization: 1011.0069580078125\n",
      "78 Evaluate Acc: 0.9566\n",
      "1100 loss: 0.18427859246730804 loss_regularization: 1039.88720703125\n",
      "1200 loss: 0.24137620627880096 loss_regularization: 1056.3935546875\n",
      "1300 loss: 0.16228419542312622 loss_regularization: 1079.1124267578125\n",
      "1400 loss: 0.23602831363677979 loss_regularization: 1079.953857421875\n",
      "1500 loss: 0.22818021476268768 loss_regularization: 1054.065185546875\n",
      "78 Evaluate Acc: 0.962\n",
      "1600 loss: 0.23845016956329346 loss_regularization: 1057.6170654296875\n",
      "1700 loss: 0.18158239126205444 loss_regularization: 1028.37841796875\n",
      "1800 loss: 0.23202309012413025 loss_regularization: 1028.9544677734375\n",
      "1900 loss: 0.15955109894275665 loss_regularization: 1049.6339111328125\n",
      "2000 loss: 0.2867510914802551 loss_regularization: 1077.757568359375\n",
      "78 Evaluate Acc: 0.9643\n",
      "2100 loss: 0.3654785454273224 loss_regularization: 1086.9796142578125\n",
      "2200 loss: 0.23683947324752808 loss_regularization: 1127.5301513671875\n",
      "2300 loss: 0.2879611849784851 loss_regularization: 1067.81591796875\n",
      "2400 loss: 0.2575156092643738 loss_regularization: 1091.1146240234375\n",
      "2500 loss: 0.2844315469264984 loss_regularization: 1066.8475341796875\n",
      "78 Evaluate Acc: 0.966\n",
      "2600 loss: 0.3126527965068817 loss_regularization: 1124.710693359375\n",
      "2700 loss: 0.24524933099746704 loss_regularization: 1117.3143310546875\n",
      "2800 loss: 0.23993903398513794 loss_regularization: 1121.7550048828125\n",
      "2900 loss: 0.20683743059635162 loss_regularization: 1093.51318359375\n",
      "3000 loss: 0.22238755226135254 loss_regularization: 1117.9136962890625\n",
      "78 Evaluate Acc: 0.9677\n",
      "3100 loss: 0.3805146813392639 loss_regularization: 1119.991943359375\n",
      "3200 loss: 0.304075688123703 loss_regularization: 1147.00048828125\n",
      "3300 loss: 0.2158583551645279 loss_regularization: 1113.6829833984375\n",
      "3400 loss: 0.2274985909461975 loss_regularization: 1115.6417236328125\n",
      "3500 loss: 0.1761568784713745 loss_regularization: 1117.2501220703125\n",
      "78 Evaluate Acc: 0.9626\n",
      "3600 loss: 0.16861671209335327 loss_regularization: 1134.2830810546875\n",
      "3700 loss: 0.1913648247718811 loss_regularization: 1121.7767333984375\n",
      "3800 loss: 0.2711109519004822 loss_regularization: 1101.7034912109375\n",
      "3900 loss: 0.21761943399906158 loss_regularization: 1063.840087890625\n",
      "4000 loss: 0.23914769291877747 loss_regularization: 1083.0089111328125\n",
      "78 Evaluate Acc: 0.9626\n",
      "4100 loss: 0.44634532928466797 loss_regularization: 1085.3935546875\n",
      "4200 loss: 0.1932133138179779 loss_regularization: 1064.297607421875\n",
      "4300 loss: 0.23352879285812378 loss_regularization: 1075.43701171875\n",
      "4400 loss: 0.1553250104188919 loss_regularization: 1070.88232421875\n",
      "4500 loss: 0.2768731713294983 loss_regularization: 1093.24267578125\n",
      "78 Evaluate Acc: 0.9531\n",
      "4600 loss: 0.13895589113235474 loss_regularization: 1102.4659423828125\n"
     ]
    }
   ],
   "source": [
    "for step,(x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = tf.reshape(x,(-1,28*28))\n",
    "        out = network(x)\n",
    "\n",
    "        y_onehot = tf.one_hot(y,depth=10)\n",
    "\n",
    "        # 计算loss值\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot,out,from_logits=True))\n",
    "        # 添加正则化\n",
    "        loss_regularization = []\n",
    "\n",
    "        # 计算正则化的值\n",
    "        # 正则化1：计算每个参数的l2值\n",
    "        for p in network.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "        # 正则化2：累加所以的参数并求平均值\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "        # 正则化3：将这个惩罚因子添加到loss function中\n",
    "        loss = loss + 0.0001 * loss_regularization\n",
    "\n",
    "    grads = tape.gradient(loss,network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n",
    "\n",
    "    # evaluate\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct = 0., 0\n",
    "\n",
    "        for step, (x, y) in enumerate(ds_val): \n",
    "            # [b, 28, 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = network(x) \n",
    "            # [b, 10] => [b] \n",
    "            pred = tf.argmax(out, axis=1) \n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # bool type \n",
    "            correct = tf.equal(pred, y)\n",
    "            # bool tensor => int tensor => numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "\n",
    "        print(step, 'Evaluate Acc:', total_correct/total)\n",
    "\n",
    "        \n"
   ]
  }
 ]
}