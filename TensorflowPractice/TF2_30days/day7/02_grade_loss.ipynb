{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e17394781024da1f0ed279aab7c6165e80e3acfd2491de02c017c43da5039587"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "source": [
    "MSE loss 的梯度计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w:  tf.Tensor(\n[[ 0.0678237  -0.02814421]\n [ 0.07160579 -0.02971363]\n [-0.15706988  0.06517791]], shape=(3, 2), dtype=float32)\nb grad: tf.Tensor([ 0.14647852 -0.06078291], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([1,3])\n",
    "w = tf.ones([3,2])\n",
    "b =tf.ones([2])\n",
    "\n",
    "y = tf.constant([0,1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w,b])\n",
    "    z = x@w+b\n",
    "    logits = tf.sigmoid(z)\n",
    "    loss = tf.reduce_mean(tf.losses.MSE(y,logits))\n",
    "\n",
    "grads = tape.gradient(loss,[w,b])\n",
    "print(\"w: \",grads[0])\n",
    "print(\"b grad:\",grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 4)\n",
      "(4, 4)\n",
      "w:  tf.Tensor(\n",
      "[[-0.0749816  -0.14142123  0.12692438  0.08947845]\n",
      " [-0.06336007  0.26747727  0.0958017  -0.29991895]\n",
      " [ 0.07344081  0.19654597 -0.11238126 -0.15760551]], shape=(3, 4), dtype=float32)\n",
      "b grad: tf.Tensor([0. 0. 0. 0.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal([4,3])\n",
    "w = tf.ones([3,4])\n",
    "b =tf.ones([4])\n",
    "# 分成4个类型\n",
    "y = tf.constant([3,1,2,0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w,b])\n",
    "    logits = x@w+b\n",
    "    # print(logits.shape)\n",
    "    y_hot = tf.one_hot(y,depth=4)\n",
    "    # print(y_hot.shape)\n",
    "    # 这里需要注意，在使用交叉熵的时候，要注意参数from_logits的设置\n",
    "    # 因为没有做softmax这一步，也就是没有转换成概率这一步\n",
    "    # 这里使用这个参数后，该函数会自动实现这个转换\n",
    "    loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_hot,logits,from_logits=True))\n",
    "\n",
    "grads = tape.gradient(loss,[w,b])\n",
    "print(\"w: \",grads[0])\n",
    "print(\"b grad:\",grads[1])"
   ]
  }
 ]
}