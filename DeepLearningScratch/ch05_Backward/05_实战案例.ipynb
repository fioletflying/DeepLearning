{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络反向传播\n",
    "\n",
    "### 神经网络学习的整个流程\n",
    "主要有以下4个步骤：\n",
    "\n",
    "- 前提：神经网络已经有合适的权重和偏置值。\n",
    "- 选取输入数据：mini-batch从数据中随机选取一部分数据进行训练\n",
    "- 计算梯度：计算各个权重参数的梯度\n",
    "- 更新参数：将权重参数沿着梯度方向进行更新\n",
    "- 重复上面的1-3步\n",
    "\n",
    "这里我们使用误差反向传播的方案来进行梯度计算，可以大大提高学习的效率。\n",
    "\n",
    "### 神经网络反向传播的实践\n",
    "\n",
    "这里接着上一章节来学习，两层神经网络的实践，主要涉及到变量如下：\n",
    "- params: 保存神经网络的参数的字典型变量\n",
    "    - params['W1']  是第 1 层的权重，\n",
    "    - params['b1']  是第 1 层的偏置。\n",
    "    - params['W2']  是第 2 层的权重， \n",
    "    - params['b2']  是第 2层的偏置\n",
    "- layers:保存神经网络的层的有序字典型变量。\n",
    "    - 以  layers['Affine1'] 、 \n",
    "    - layers['ReLu1'] 、\n",
    "    - layers['Affine2']的形式，通过有序字典保存各个层\n",
    "    \n",
    "主要的方法：\n",
    "- __init__(self, input_size, hidden_size, output_size, weight_init_std)进行初始化\n",
    "- predict(self, x)进行识别\n",
    "- loss(self, x, t) 计算损失函数的值。\n",
    "- accuracy(self, x, t) 计算识别精度\n",
    "- gradient(self, x, t) 通过误差反向传播法计算关于权重参数的梯度\n",
    "\n",
    "下面coding 实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 函数的改进\n",
    "import numpy as np\n",
    "def softmax(a):\n",
    "    c= np.max(a)\n",
    "    b = a - c # 防止溢出\n",
    "    exp_a = np.exp(b)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    # 这里引入reshape变换\n",
    "    #为了方便计算batch_size的输入\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量数据Affine层的class\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        \n",
    "    def backward(self,dout=1):\n",
    "        #获取批量数据的大小\n",
    "        batch_size = self.t.shape[0]\n",
    "        # 需要平均化最后的结果\n",
    "        dx = (self.y-self.t)/batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # 将x中大于零的变为true,小于零变为False\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        #布尔类型索引,true的变为0\n",
    "        out[self.mask] = 0 \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "#from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,\n",
    "                 output_size,weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "         # 生成层\n",
    "        # 生成有序字典\n",
    "        self.layers = OrderedDict()      \n",
    "        self.layers['Affine1'] = Affine(\n",
    "            self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(\n",
    "            self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    # 预测/推理    \n",
    "    def predict(self, x):\n",
    "        # 循环执行每一层即可\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    #计算精度\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据\n",
    "\n",
    "接下来就是利用反向传播来对数据进行学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.19251666666666667, 0.1972\n",
      "train acc, test acc | 0.09751666666666667, 0.0974\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.10218333333333333, 0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in subtract\n",
      "  \"\"\"\n",
      "d:\\program files\\python35\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in less_equal\n",
      "  import sys\n",
      "d:\\program files\\python35\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n",
      "train acc, test acc | 0.09871666666666666, 0.098\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 加载数据\n",
    "(x_train,y_train),(x_test,y_test) = load_mnist(\n",
    "                normalize=True,\n",
    "                one_hot_label=True)\n",
    "\n",
    "# 记录每次迭代的loss值\n",
    "train_lost_list = []\n",
    "train_acc_list = [] #记录训练精度\n",
    "test_acc_list = [] #记录测试精度\n",
    "\n",
    "\n",
    "#设置训练中参数\n",
    "iters_num = 10000 # 训练次数\n",
    "train_size = x_train.shape[0] #样本数量\n",
    "batch_size = 100 #mini-batch的数量\n",
    "learning_rate = 0.1 # 学习率\n",
    "\n",
    "# 需要训练多次达到一个epoch\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "#定义一个两层的网络\n",
    "# 隐藏层为50\n",
    "network = TwoLayerNet(input_size=784,\n",
    "                     hidden_size=50,\n",
    "                     output_size=10)\n",
    "\n",
    "# 开始训练\n",
    "for i in range(iters_num):\n",
    "    # 随机选取输入的数据，选取的量为100个\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.gradient(x_batch,y_batch)\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    #记录loss值\n",
    "    loss = network.loss(x_batch,y_batch)\n",
    "    train_lost_list.append(loss)\n",
    "    \n",
    "    if i%iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,y_train)\n",
    "        test_acc = network.accuracy(x_test,y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f34c730160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot  as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(test_acc_list)\n",
    "plt.plot(train_acc_list,linestyle=\"--\")\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
