{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine&Softmax层的实现\n",
    "\n",
    "### Affine层\n",
    "\n",
    "这里的affine层其实就是说神经网络中的，信号加权的计算过程，也就是XW+B这个公式的计算。\n",
    "\n",
    "> Affine表示仿射的意思，这是几何的概念，仿射一般是指对图形的平移或者线性伸缩。也就对应这里的对于输入信号的权重的线性变换和偏置的平移变换。\n",
    "\n",
    "如下coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) [0.03522538 0.33746387]\n",
      "(2, 3) [[0.71601237 0.59287577 0.8510641 ]\n",
      " [0.57232106 0.74278995 0.62142448]]\n",
      "(3,) [0.68368357 0.8806054  0.13857277]\n",
      "(3,) [0.90204306 1.15215445 0.37826014]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 输入\n",
    "X = np.random.rand(2)\n",
    "# 权重 W\n",
    "W = np.random.rand(2,3)\n",
    "# 偏置值 B\n",
    "B = np.random.rand(3)\n",
    "\n",
    "print(X.shape,X)\n",
    "print(W.shape,W)\n",
    "print(B.shape,B)\n",
    "\n",
    "Y = np.dot(X,W) + B\n",
    "print(Y.shape,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的例子中我们可以看到的数据是,以上的数据都是向量或者矩阵。Y是经过矩阵的点乘获得的，其矩阵变换如下：\n",
    "![](imgs/19.jpg)\n",
    "\n",
    "计算图如下：\n",
    "![](imgs/20.jpg)\n",
    "\n",
    "上面所有的数据都是多维数组的格式。\n",
    "\n",
    "反向传播的过程中，对于向量的点乘t运算，其实这个也就是相当于标量的乘法求导，这里要注意的是：\n",
    "- 向量求导要注意两个数据的位置\n",
    "- 其次是要注意需要转置\n",
    "计算图如下：\n",
    "![](imgs/21.jpg)\n",
    "\n",
    "这里有一个推导的过程如下：\n",
    "![](imgs/22.jpg)\n",
    "\n",
    "### 批量数据集的Affine层\n",
    "\n",
    "上面的公式是单个数据集输入的时候反向传播的过程层。现在来看看，如果是批量数据我们又将如何来处理。\n",
    "![](imgs/23.jpg)\n",
    "\n",
    "对比刚刚上面的变换，dW的shape是没有任何变化的，主要的变换是在X的数据变成了N,这样反向传播过来的数据也在这个维度数量上变成了N。\n",
    "\n",
    "上面看到关于dW的权重的计算过程和公式，现在来分析一下dB是如何计算的。\n",
    "B正向传播时，会根据批量数据的个数N，利用广播机制将数据自动扩展在对于的数据个数N。\n",
    "dB反向传播时，会将这N个数据汇总到偏置的元素上，因为要保证dB的shape与B统一。这里的汇总其实就是**对N个数据的导数按元素进行求和**\n",
    "先看一个coding的例子，大家来感受一下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 这个是X.W的结果\n",
    "XdotW = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "# 下面需要计算加上偏置值\n",
    "Y = XdotW + B\n",
    "# 这里虽然B是是个一维的向量\n",
    "# 但是numpy会利用广播机制\n",
    "# 将B扩展为（2，3）\n",
    "# np.array([[1,2,3],[1,2,3]])\n",
    "print(Y)\n",
    "\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "# axis=0,对第0个轴方向上的元素进行求和\n",
    "dB = np.sum(dY,axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量数据Affine层的class\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W) + b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sofmax-with_Loss 层\n",
    "\n",
    "前面几个小节中我们看到了关于激活函数反向传播（sigmoid/ReLU）以及刚刚看到的Affine层也就时全连接层的反向传播中计算的(dW,db),现在我们来看看组成神经网络的最后一层时如何进行反向传播的。\n",
    "比如分类的话，我们一般使用softmax来进行概率评估。\n",
    "如下图所示：\n",
    "![](imgs/24.jpg)\n",
    "\n",
    "这里我们要注意两个概念：推理与学习\n",
    "\n",
    ">深度学习中的推理（inference）如果是分类问题，是给一个数据来判断该数据是属于哪一类，如果是回归问题，是给一个数据来预测该数据的结果是多少。所以这个过程只需要给出结果即可。一般是在神经网络最后一层中获得输出中获得一个结果。所以这里可能不需要计算softmax层，直接从Affine层给出一个答案即可。\n",
    "\n",
    ">深度学习中的学习（training）这里的学习就是训练，这个时候我们不仅需要最后一层给出结果，也需要给出具体与答案差多少。这个时候就需要softmax层对输出的做一个正规化，将结果进行概率评分，这样方便我们下一步的反向传播，来调整参数使得模型更加的符合我们预期的输出。\n",
    "\n",
    "好了上面介绍学习过程，这里需要我们给出与预期值的误差。这个时候我们就需要损失函数了。我们先从cross entropy error来分析。交叉熵误差是我们经常要用的loss function。该函数的计算图如下:\n",
    "\n",
    "![](imgs/25.jpg)\n",
    "\n",
    "上面的有点复杂，下面来看看简化版本\n",
    "![](imgs/26.jpg)\n",
    "\n",
    "在softmax-loss反向传播最终得到的结果是$(y_1-t_1,y_2-t_2,y_3-t_3)$ 这个不就是训练值与目标值的误差吗？这样通过这个误差在反向传播来修改和更新对应的参数。\n",
    "\n",
    "下面coding如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 函数的改进\n",
    "import numpy as np\n",
    "def softmax(a):\n",
    "    c= np.max(a)\n",
    "    b = a - c # 防止溢出\n",
    "    exp_a = np.exp(b)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    # 这里引入reshape变换\n",
    "    #为了方便计算batch_size的输入\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7))/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        \n",
    "    def backward(self,dout=1):\n",
    "        #获取批量数据的大小\n",
    "        batch_size = self.t.shape[0]\n",
    "        # 需要平均化最后的结果\n",
    "        dx = (self.y-self.t)/batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
