{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重的初始值\n",
    "\n",
    "权重的是初始化的好坏会影响到神经网络学习效果和效率。\n",
    "先剧透一下，很多时候我们利用权重衰减来抑制过拟合和提高泛化能力。\n",
    "所谓的衰减就是减少权重参数的值。\n",
    "\n",
    "### 权重都设置为0的后果\n",
    "\n",
    "我们在之前的案例中，第一步就是对参数进行进行初始化，而且使用的函数如下：\n",
    "`0.01 * np.random.randn(10, 100) ` \n",
    "- randn：表示使用高斯分布（0，1）\n",
    "- 乘以0.01：将标准差设置为0.01\n",
    "\n",
    "那么这里我们要问一个极端的问题，既然要保证初始化值尽可能的小，那么我们干脆将所有的参数初始化为0.\n",
    "\n",
    "- 如果所有的参数初始化为0，导致无法正确学习\n",
    "- 首先如果正向传播会导致结果出现极端的情况，要么为0，-1\n",
    "- 反向传播的过程中需要使用前一层的参数，导致传递值相同，所以权重的更新值也一样的\n",
    "- 权重均一化，使得神经网络的不同的权重意义丧失了\n",
    "\n",
    "### 隐藏层的激活值的分布\n",
    "\n",
    "，我们来做一个简单的实验，观察权重初始值是如何影响隐藏层的激活值的分\n",
    "布的。这里要做的实验是，向一个 5 层神经网络（激活函数使用 sigmoid 函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布。\n",
    "\n",
    "具体的coding如下图：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000个数据\n",
    "node_num = 100  # 各隐藏层的节点（神经元）数\n",
    "hidden_layer_size = 5  # 隐藏层有5层\n",
    "activations = {}  # 激活值的结果保存在这里\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 改变初始值进行实验！\n",
    "    #w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 将激活函数的种类也改变，来进行实验！\n",
    "    # z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 绘制直方图\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/12.jpg)\n",
    "从上图中我们可以看出：\n",
    "- 各层的激活值呈偏向 0 和 1 的分布\n",
    "- 由于sigmoid函数为S型，越接近0或1，梯度接近为0\n",
    "- 这样会数据集中到0,1端，导致梯度越来越小，最终出现梯度消失的问题\n",
    "\n",
    "为了避免这种情况，我们使用0.01的标准差的高斯分布，如下图。\n",
    "![](imgs/13.jpg)\n",
    "\n",
    "从上面的图中我们可以看到：\n",
    "- 呈集中在 0.5 附近的分布，不会出现梯度消失的问题\n",
    "- 随着深度的增加，其值越来越聚集到0.5左右\n",
    "- 这样的激活值就会出现偏向的问题，也就是说神经元输出的值几乎相同\n",
    "- 网络的表现力受限，这样就会出现使用100个神经元与使用一个神经元的效果一样\n",
    "- 这样的结果也不是我们想要，激活值得广度不够。\n",
    "\n",
    "### Xavier 初始值\n",
    "\n",
    "为了解决以上问题， Xavier Glorot 等人的论文中推荐的权重初始值，为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。\n",
    "- 如果前一层的节点数为 n，则初始值使用标准差为$\\frac1{\\sqrt{n}}$的分布  \n",
    "\n",
    "使用 Xavier 初始值后，前一层的节点数越多，要设定为目标节点的初始值的权重尺度就越小。\n",
    "\n",
    "实验的结果如下图：\n",
    "![](imgs/14.jpg)\n",
    "\n",
    "其结论如下：\n",
    "- 越是后面的层，图像变得越歪斜\n",
    "- 呈现了比之前更有广度的分布\n",
    "-  sigmoid 函数的表现力不受限制，有望进行高效的学习\n",
    "\n",
    "为了更好的效果，我们这里可以看出sigmoid函数出现一定的倾斜，但是使用了tanh激活函数后，会得到一定的改善的。具体如下图所示。\n",
    "![](imgs/15.jpg)\n",
    "\n",
    "- 使用 tanh 函数后，会呈漂亮的吊钟型分布\n",
    "- tanh 函数是关于原点 (0, 0) 对称的S 型曲线\n",
    "- 用作激活函数的函数最好具有关于原点对称的性质\n",
    "\n",
    "所以一般如果除非是需要进行二值分类的问题，一般激活函数我们选用tanh激活函数。\n",
    "\n",
    "\n",
    "### ReLU的权重初始值\n",
    "\n",
    "上面介绍的Xavier初始化算法是针对线性函数提出的，因为 sigmoid\n",
    "函数和 tanh 函数左右对称，且中央附近可以视作线性函数。\n",
    "\n",
    "现在比较流行的激活函数relu是非线性的，所以需要使用更好的方法，Kaiming He 等人推荐的初始值，也称为“He 初始值”\n",
    "![](imgs/16.jpg)\n",
    "- 当前一层的节点数为 n 时，He 初始值使用标准差为 $\\sqrt{\\frac2{n}}$的高斯分布\n",
    "-  ReLU 的负值区域的值为 0,，为了使它更有广度，所以需要 2 倍的系数\n",
    "\n",
    "总结一下，这是目前的最佳实践:\n",
    "- 当激活函数使用 ReLU 时，权重初始值使用 He 初始值，\n",
    "- 当激活函数为 sigmoid 或 tanh 等 S 型曲线函数时，初始值使用 Xavier 初始值。\n",
    "\n",
    "### 基于 MNIST 数据集的权重初始值的比较\n",
    "\n",
    "我们直接来看图：\n",
    "![](imgs/17.jpg)\n",
    "\n",
    "- std = 0.01 时完全无法进行学习，因为正向传播中传递的值很小\n",
    "- 当权重初始值为 Xavier 初始值和 He 初始值时，学习进行得很顺利\n",
    "- He 初始值时的学习进度更快一些\n",
    "\n",
    "综上：在神经网络的学习中，权重初始值非常重要。很多时候权重初始值的设定关系到神经网络的学习能否成功。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
